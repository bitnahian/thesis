\chapter{Introduction}

\section{Motivation}
Distributed and parallel database systems have become a corner stone of large-scale data warehousing and data analysis. A \textbf{parallel database system} improves on paralellising operations such as loading data, building indexes and evaluating queries \cite{Ramakrishnan:2002:DMS:560733}. In a \textbf{distributed database system}, data is physically spread across different sites, and each site is typically managed by a DBMS capable of running independently. In such systems, the locality of data significantly impacts query optimisation and processing, concurrency control and recovery. The distribution of data is primarily motivated by three factors - i) \textbf{increased availability of data}, ii) \textbf{distributed access to data} and iii) \textbf{analysis of data} \cite{Ramakrishnan:2002:DMS:560733}. 

The data running in applications in a distributed database system require to be placed across different sites. The two basic forms of placing data are: i) \textbf{partitioned} and ii) \textbf{replicated}. In the partitioned scheme the database is divided into a number of disjoint partitions which are then placed at different sites. Replicated designs can either be \textbf{fully replicated}, where the entire database is copied at each location or \textbf{partially replicated}, where only certain partitions of the database are selectively chosen to be replicated across multiple sites, but not all of them \cite{ozsu2011principles}. 

Commercial products such as Microsoft Azure SQL Data Warehouse or Amazon Redshift provide ready-to-use scale-out database solutions for OLAP-style workloads in the \textbf{cloud}. Because of the aforementioned reasons, customers need to make important design decisions for the physical data layout, such as selecting the partitioning schemes for the data which are traditionally made by database administrators (DBAs). However, finding and defining suitable partitioning schemes is a difficult task even for experienced DBAs. This is because the DBA has to understand the physical query execution for the specific query workloads in order to optimise the performance of the DBMS \cite{jindal2011relax}. 

Partitioning a database optimally is a non-trivial task and hence different partitioning schemes can significantly improve the overall performance. For example, analytical queries often involve multiple joins over large relations. When two relations which are co-partitioned on the same join attribute need to be joined, computation costs can be significantly reduced if the join operation is executed locally on the node if both the relations are partitioned on the same node - thus removing the cost of expensive network  transfers. Finding the optimal partitioning scheme for a complex schemata which involve multiple co-partitioning strategies and join paths is a non-trivial task since it not only depends on the database schemata but also other factors such as table sizes, the query workload (i.e., which joins are actually important and how often tables are joined), or hardware characteristics such as network speed. 

Historically, a lot of research in finding the best partition schemes has involved mathematical programming in order to minimise the combined cost of storing the database, processing transactions against it, and message communication among sites and the general problem is considered to be an NP-hard problem \cite{ozsu2011principles}. As such, most proposed solutions are based on heuristics \cite{navathe1984vertical, agrawal2004integrating, agrawal2006automatic, bruno2007online, bruno2008constrained, kimura2010coradd, ozmen2010workload, curino2010schism, quamar2013sword, DBLP:conf/sigmod/ZhouBL12, DBLP:conf/sigmod/NehmeB11}. However, these techniques typically rely on simple heuristics to find a partitioning scheme that work only for simple star-schema-like databases or they rely on cost estimates from the query optimiser and find a solution by considering finding the partitioning scheme as an optimisation problem. As such, the advisor not only relies on often inaccurate cost estimates which do not reflect the real execution costs for complex multi-join queries but more importantly the advisor cannot easily generalise to new (unseen) workloads without re-running the optimisation procedure for every new workload or mix. 
\section{Contributions}
\label{contribs}
In this thesis, we explore \texttt{RL4DBPartitioning}, where we make a case for Deep Reinforcement Learning (DRL) for learning an advisor that can suggest good database partitioning schemes. In this thesis, we expand upon the work performed by \citeauthor{Hilprecht:2019:TLP:3329859.3329876} in \cite{Hilprecht:2019:TLP:3329859.3329876}. They successfully implement and deploy a Deep Reinforcement Learning advisor, which is not only capable of finding partitions that outperform existing approaches for automated data partitioning design, but that it can also easily adjust to different deployments. This approach is discussed in further detail in Section~\ref{db_partitioning}. 

To summarise, we aim to extend the DRL agent by making the following contributions:
\begin{itemize}
    \item We extend the online model developed by \citeauthor{Hilprecht:2019:TLP:3329859.3329876} to consider compound key partitioning schemes as opposed to just single key partitioning schemes.
    \item We want to extend the partitioning agent to be more \textbf{query-aware} as opposed to just learning from monitoring the rewards for different workloads.
    \item We would like to evaluate the effectiveness of the new models against heuristic based approaches and test it under synthetic benchmarks. 
\end{itemize}

\section{Outline}
TODO
